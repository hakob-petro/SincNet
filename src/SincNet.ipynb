{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf461ba",
   "metadata": {},
   "source": [
    "## Распознавание говорящего по необработанной форме сигнала с помощью SincNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc9637",
   "metadata": {},
   "source": [
    "* Автор нотбука: Петросян Акоб\n",
    "* МФТИ, г. Долгопрудный, Московская область\n",
    "* Обратная связь: akob.petrosyan@phystech.edu, [vk.com/jacpetro](vk.com/jacpetro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723542a9",
   "metadata": {},
   "source": [
    "Сегодня нам предстоит изучать новую архитектуру CNN, названная SincNet.  \n",
    "Подробное описание архитектуры доступно по ссылке: [SincNet](https://arxiv.org/pdf/1808.00158.pdf)\n",
    "<br>  \n",
    "Идея метода состоит в том, что в первом свёрточном слое мы используем функцию  $sinc(x) = \\frac{sin(x)}{x}$, или, что одно и тоже в случае частотной зависимости, - полосовые фильтры. С помощью этих функций считаем свертки c входными сигналами $y[n] = x[n]*g_{w}[n, f1, f2]$, где   \n",
    "<br>\n",
    "<center>$g_{w}[n, f1, f2] = g[n, f1, f2]w[n]$</center>  \n",
    "<br>\n",
    "<center>$g[n, f1, f2] = 2f_{2}sinc(2\\pi f_{2}n) − 2f_{1}sinc(2\\pi f_{1}n)$  </center>   \n",
    "<br>\n",
    "<center>$w[n] = 0.54 − 0.46cos(\\frac{2\\pi n}{L})$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4e618",
   "metadata": {},
   "source": [
    "Преимушества токого подхода:\n",
    "1. Быстрая сходимость: : SincNet заставляет сеть фокусироваться только на параметрах фильтра, что существенно влияет на производительность\n",
    "2. Мало параметров: $2F$ вместо $2FL$, где $F$ количество фильтров, а $L$ длина фильтров\n",
    "3. Интерпретируемость: Карты признаков SincNet, полученные в первом сверточном слое, более интерпретируемы и удобочитаемы, чем другие подходы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982e863",
   "metadata": {},
   "source": [
    "![Architecture of SincNet](https://miro.medium.com/max/1266/1*sUoYw3qewfZVrcBl6RIZRQ.png (SincNet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef7a23",
   "metadata": {},
   "source": [
    "## А теперь в БОЙ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "0738c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "\n",
    "import torchsummary\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from matplotlib import colors, pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# в sklearn не все гладко, поэтому мы будем игнорировать warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "67f306cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "b9ee069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разные режимы датасета \n",
    "DATA_MODES = ['train', 'val', 'test']\n",
    "# некоторые константы\n",
    "NUMBER_OF_FILTERS = 80\n",
    "WAVETIME = 0.2 # 200 мс\n",
    "L = 251\n",
    "# при возможности работаем на видеокарте\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# torch pi...\n",
    "PI = torch.from_numpy(np.array(np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8425df",
   "metadata": {},
   "source": [
    "Ниже мы исспользуем враппер над датасетом для удобной работы.  \n",
    "Стоит также отметить, что мы переопределяем метод __getitem__ для удобства работы с данной структурой данных.\n",
    "Также используем LabelEncoder для преобразования строковых меток классов в id и обратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "254bffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SincNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с записями, который паралельно подгружает их из папок\n",
    "    производит разделение на маленькие куски, скалирование и превращение в торчевые тензоры\n",
    "    \"\"\"\n",
    "    def __init__(self, files, mode, wavetime, trashhold = 0.01):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # продолжительность сигналов в секунах\n",
    "        self.wavetime = wavetime\n",
    "        # граница шума\n",
    "        self.trashhold = trashhold\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "     \n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        waveform, sample_rate = torchaudio.load(file.as_posix())\n",
    "        return waveform, sample_rate\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        переопределяем __getitem__ для нашего удобства\n",
    "        \"\"\"\n",
    "        x, f = self.load_sample(self.files[index])\n",
    "        x = self._prepare_sample(x, f)\n",
    "        f = [f for i in range(len(x))] # потвторяем частоту дискретизации для каждого куска\n",
    "        f = torch.FloatTensor(f)\n",
    "        if self.mode == 'test':\n",
    "            return x, f\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            y = [y for i in range(len(x))]\n",
    "            y = torch.FloatTensor(y) # потвторяем лейбл для каждого куска\n",
    "            return x, f, y\n",
    "        \n",
    "    def _prepare_sample(self, sample, sample_rate):\n",
    "        \"\"\"\n",
    "        Эта функция возврашает список 200мс-х записей, каждый элемент которого - это тензор\n",
    "        очишенный от шума, выровненный и нормализировнный.\n",
    "        \"\"\"\n",
    "        # избавляемся от шума\n",
    "        clean = sample[0][sample[0] > self.trashhold*torch.max(torch.abs(sample[0]))]\n",
    "        sample = clean[None, :]\n",
    "        number = self.wavetime * sample_rate # количесиво значений каждого разделенного сигнала\n",
    "        if len(sample[0]) > number:\n",
    "            samples = list(torch.tensor_split(sample, int(sample.shape[1] // number + 1), dim=1))\n",
    "            last_sample = samples[-1]\n",
    "            samples = samples[:-1]\n",
    "                \n",
    "            \"\"\"\n",
    "            # последний кусок скорее всего меньше по размеру чем остальные, поэтому...\n",
    "            # проверяем, если длина этого куска меньше 80 процентов длины остальных сигналов,\n",
    "            # то выбрасываем его, иначе, заполняем padding-ом с двух сторон\n",
    "            diff = number-last_sample.shape[1]\n",
    "            if diff > 0.2*number:\n",
    "                if diff%2 == 0:\n",
    "                    p1d = (int(diff/2), int(diff/2))\n",
    "                    last_sample = F.pad(last_sample, p1d, \"reflect\")\n",
    "                    samples.append(last_sample)\n",
    "                else:\n",
    "                    p1d = (int(diff//2), int(diff//2 + 1))\n",
    "                    last_sample = F.pad(last_sample, p1d, \"reflect\") \n",
    "                    samples.append(last_sample)\n",
    "            \"\"\"\n",
    "        # наконец, нормализируем данные\n",
    "        for i in range(len(samples)):\n",
    "            samples[i] = (samples[i] - torch.mean(samples[i])) / torch.std(samples[i])\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986676f8",
   "metadata": {},
   "source": [
    "Далее получим необходимые датасеты..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "abef05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(r'C:\\Users\\hakob\\OneDrive\\Рабочий стол\\KissMe\\MIPT\\VK SincNet\\data')\n",
    "train_val_test_files = sorted(list(DATA_DIR.rglob('*.flac')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "37be884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_test_labels = [path.parent.name for path in train_val_test_files]\n",
    "train_val_files, test_files = train_test_split(train_val_test_files, test_size=0.2, stratify=train_val_test_labels)\n",
    "train_val_labels = [path.parent.name for path in train_val_files]\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.25, stratify=train_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "fcfd1b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from train_files: C:\\Users\\hakob\\OneDrive\\Рабочий стол\\KissMe\\MIPT\\VK SincNet\\data\\LibriSpeech\\train-clean-100\\7264\\92316\\7264-92316-0006.flac.\n",
      "Number of train files: 17123, val files: 5708, test files: 5708.\n",
      "Number of points of one of initial waves: 246400, sample rate: 16000 Hz.\n",
      "Wall time: 7.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"Example from train_files: {train_files[0]}.\")\n",
    "print(f\"Number of train files: {len(train_files)}, val files: {len(val_files)}, test files: {len(test_files)}.\")\n",
    "waveform, sample_rate = torchaudio.load('C:/Users/hakob/OneDrive/Рабочий стол/KissMe/MIPT/VK SincNet/data/LibriSpeech/train-clean-100/909/131044/909-131044-0004.flac')\n",
    "print(f\"Number of points of one of initial waves: {waveform.shape[1]}, sample rate: {sample_rate} Hz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "6a3f7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = SincNetDataset(train_files, \"train\", 0.2)\n",
    "val_dataset = SincNetDataset(train_files, \"val\", 0.2)\n",
    "test_dataset = SincNetDataset(train_files, \"test\", 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302c60c",
   "metadata": {},
   "source": [
    "### Построение нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81246b0",
   "metadata": {},
   "source": [
    "Ниже реализована семантика SincNet.  \n",
    "None-ы используются для увеличения размерности фильтров, так как сигналы подаются пачками.\n",
    "\n",
    "Подробно:  \n",
    "- [torch.nn.functional.conv1d](https://pytorch.org/docs/stable/nn.functional.html)\n",
    "- [torch.nn.Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "49fe328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SincNet layer\n",
    "\n",
    "class Custom(nn.Module):\n",
    "    def __init__(self, number_of_filters, l):\n",
    "        super().__init__()\n",
    "        self.number_of_filters = number_of_filters\n",
    "        self.L = l\n",
    "       \n",
    "    def forward(self, x, sample_rate):\n",
    "        \"\"\"\n",
    "            x.shape = (batch_size, input channels, wavetime*sample_rate)\n",
    "            result_filters.shape = (output_shape, input channels, L)\n",
    "            result.shape = (batch_size, output_shape, wavetime*sample_rate - L + 1)\n",
    "            \n",
    "            Все расчеты выполнены при дефолтных значений параметров, а именно:\n",
    "            stride = 1\n",
    "            groups = 1\n",
    "            padding = 0\n",
    "            dilation = 1\n",
    "        \"\"\" \n",
    "        hamming_window = torch.hamming_window(self.L, device = DEVICE) # окно длины 251 (в нашем случае)\n",
    "        hamming_window = hamming_window[None, :] # shape (1, 251)\n",
    "        n = torch.FloatTensor([i for i in range(self.L)])\n",
    "        n = n[None, :] # shape (1, 251)\n",
    "        f_1 = torch.rand(self.number_of_filters, 1,  1) * sample_rate /2 # shape (80, 1, 1)\n",
    "        f_2 = f_1 + torch.abs(f_1 - torch.rand(self.number_of_filters, 1, 1) * sample_rate /2)\n",
    "        g_filter = 2*f_2*torch.sinc(2*PI*f_2*n) - 2*f_1*torch.sinc(2*PI*f_1*n) \n",
    "        result_filters = g_filter * hamming_window # в нашем случае (80, 1, 251)\n",
    "        results = F.conv1d(x, result_filters) # (batch_size, 1, 0.2*sample_rate) * (80, 1, 251) = (batch_size, 80, 0.2*sample_rate - 250)\n",
    "                \n",
    "        return resultsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "f51ec1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SincNet\n",
    "class SincNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        def _Glorot_weights(m):\n",
    "            if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        \n",
    "        # SincNet layer\n",
    "        self.custom = nn.Sequential(\n",
    "            Custom(NUMBER_OF_FILTERS, L), # out_channels = 80\n",
    "            nn.MaxPool1d(kernel_size=2), # out_channels = 40\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2) # out_channels = 40, shape: (batch_size, 40, 0.2*sample_rate - 250)\n",
    "        )\n",
    "        # standard CNN 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=40, out_channels=60, kernel_size=5),\n",
    "            nn.ReLU() # shape: (batch_size, 60, 0.2*sample_rate - 254) \n",
    "        )\n",
    "        self.conv1.apply(_Glorot_weights)\n",
    "        # standard CNN 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=60, out_channels=60, kernel_size=5),\n",
    "            nn.ReLU() # shape: (batch_size, 60, 0.2*sample_rate - 258) \n",
    "        )\n",
    "        self.conv2.apply(_Glorot_weights)\n",
    "        # Dense layer 1\n",
    "        self.lin1 = nn.Sequential(\n",
    "            nn.Linear(in_features = int(60*(0.2*16000 - 258)), out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048)\n",
    "        )\n",
    "        self.lin1.apply(_Glorot_weights)\n",
    "        # Dense layer 2\n",
    "        self.lin2 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048)\n",
    "        )\n",
    "        self.lin2.apply(_Glorot_weights)\n",
    "        # Dense layer 3\n",
    "        self.lin3 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048)\n",
    "        )\n",
    "        self.lin3.apply(_Glorot_weights)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=2048, out_features=n_classes)\n",
    "    \n",
    "    def forward(self, x, sample_rate):\n",
    "        x = self.custom(x, sample_rate)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "27f061ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs, sample_rate, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "e3d0d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "903fd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры оттимайзера и размер батча\n",
    "LR = 0.001\n",
    "ALPHA = 0.95\n",
    "EPS = 10-7\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "9591e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_files, val_files, model, epochs, batch_size):\n",
    "    train_loader = DataLoader(TensorDataset(train_dataset), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        opt = torch.optim.RMSprop(model.parameters(), lr=LR, alpha=ALPHA, eps=EPS)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "18549f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "251b27c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-532-2a5a3da59adf>:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will classify :585\n",
      "SincNet(\n",
      "  (custom): Sequential(\n",
      "    (0): Custom()\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(40, 60, kernel_size=(5,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(60, 60, kernel_size=(5,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (lin1): Sequential(\n",
      "    (0): Linear(in_features=176520, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (lin2): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (lin3): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (out): Linear(in_features=2048, out_features=585, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(train_val_labels))\n",
    "net = SincNet(n_classes).to(DEVICE)\n",
    "print(\"we will classify :{}\".format(n_classes))\n",
    "print(simple_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d3b07",
   "metadata": {},
   "source": [
    "## Запустим обучение!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6bff0d",
   "metadata": {},
   "source": [
    "![ALt Text](https://media.tenor.com/images/2471505153e8225ff6940afc65fba1f8/tenor.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "f84f7d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SincNetDataset' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-600-3c972a067a19>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_files, val_files, model, epochs, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mval_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Size mismatch between tensors\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Size mismatch between tensors\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SincNetDataset' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train(train_dataset, val_dataset, model=SincNet, epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964a10e",
   "metadata": {},
   "source": [
    "Построим кривые обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3959fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20929cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef97f3",
   "metadata": {},
   "source": [
    "## А теперь предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7585c4",
   "metadata": {},
   "source": [
    "![alt text](https://www.vokrug.tv/pic/product/3/2/c/b/32cb49b46b5988307a2fab35e4a5de1e.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
    "probs = predict(simple_cnn, test_loader)\n",
    "\n",
    "\n",
    "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76beab",
   "metadata": {},
   "source": [
    "Вот и всё.)  \n",
    "\n",
    "Спасибо за внимание!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
